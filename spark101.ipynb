{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d111385d-c1d9-4667-b7ac-410e747b2fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import asc, desc\n",
    "from pyspark.sql.functions import month, year, quarter\n",
    "\n",
    "from pydataset import data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f4b4ee-e3d9-4d74-b29f-7e3f36c1d210",
   "metadata": {},
   "source": [
    "# Spark 101 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9435b3ee-484b-45fb-b192-1fb26ee57f89",
   "metadata": {},
   "source": [
    "## Create a spark data frame that contains your favorite programming languages.\n",
    "\n",
    "* The name of the column should be language\n",
    "* View the schema of the dataframe\n",
    "* Output the shape of the dataframe\n",
    "* Show the first 5 records in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0b7e066-3fcd-4d88-810f-2bbf771b9778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/30 13:43:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame:  (6, 1)\n",
      "+----------+\n",
      "|  language|\n",
      "+----------+\n",
      "|    Python|\n",
      "|      Java|\n",
      "|JavaScript|\n",
      "|       C++|\n",
      "|      Ruby|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"ProgrammingLanguages\").getOrCreate()\n",
    "\n",
    "# Define the data\n",
    "data = [(\"Python\",),\n",
    "        (\"Java\",),\n",
    "        (\"JavaScript\",),\n",
    "        (\"C++\",),\n",
    "        (\"Ruby\",),\n",
    "        (\"Go\",)]\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, [\"language\"])\n",
    "\n",
    "# View the schema of the DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Output the shape of the DataFrame\n",
    "print(\"Shape of the DataFrame: \", (df.count(), len(df.columns)))\n",
    "\n",
    "# Show the first 5 records in the DataFrame\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4607762b-6745-445b-ab2c-9ab991a803d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[manufacturer: string, model: string, displ: double, year: bigint, cyl: bigint, trans: string, drv: string, cty: bigint, hwy: bigint, fl: string, class: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ae0f738-6b78-4b03-abc6-7e664c6267a8",
   "metadata": {},
   "source": [
    "## 2. Load the mpg dataset as a spark dataframe.\n",
    "\n",
    "Create 1 column of output that contains a message like the one below:\n",
    "\n",
    "\n",
    "The 1999 audi a4 has a 4 cylinder engine.\n",
    "For each vehicle.\n",
    "\n",
    "Transform the trans column so that it only contains either manual or auto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e848f54-8194-474e-9f7b-7a7e779c1e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "mpg_df = spark.createDataFrame(data('mpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2fddfed-001f-479a-953d-43d39607752a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|manufacturer|model|displ|year|cyl|     trans|drv|cty|hwy| fl|  class|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|        audi|   a4|  1.8|1999|  4|  auto(l5)|  f| 18| 29|  p|compact|\n",
      "|        audi|   a4|  1.8|1999|  4|manual(m5)|  f| 21| 29|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|manual(m6)|  f| 20| 31|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|  auto(av)|  f| 21| 30|  p|compact|\n",
      "|        audi|   a4|  2.8|1999|  6|  auto(l5)|  f| 16| 26|  p|compact|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "371d61f2-447c-4742-a1b4-1c21c65316e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----+---+------+---+---+---+---+-------+-----------------------------------------+\n",
      "|manufacturer|model|displ|year|cyl|trans |drv|cty|hwy|fl |class  |output                                   |\n",
      "+------------+-----+-----+----+---+------+---+---+---+---+-------+-----------------------------------------+\n",
      "|audi        |a4   |1.8  |1999|4  |auto  |f  |18 |29 |p  |compact|The 1999 audi a4 has a 4 cylinder engine.|\n",
      "|audi        |a4   |1.8  |1999|4  |manual|f  |21 |29 |p  |compact|The 1999 audi a4 has a 4 cylinder engine.|\n",
      "|audi        |a4   |2.0  |2008|4  |manual|f  |20 |31 |p  |compact|The 2008 audi a4 has a 4 cylinder engine.|\n",
      "|audi        |a4   |2.0  |2008|4  |auto  |f  |21 |30 |p  |compact|The 2008 audi a4 has a 4 cylinder engine.|\n",
      "|audi        |a4   |2.8  |1999|6  |auto  |f  |16 |26 |p  |compact|The 1999 audi a4 has a 6 cylinder engine.|\n",
      "+------------+-----+-----+----+---+------+---+---+---+---+-------+-----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat, lit, regexp_extract, when\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"MPG\").getOrCreate()\n",
    "\n",
    "# Assuming you have already loaded the DataFrame as 'mpg_df'\n",
    "\n",
    "# Create a new column 'output' with the desired message\n",
    "mpg_df = mpg_df.withColumn(\"output\", concat(\n",
    "    lit(\"The \"),\n",
    "    mpg_df.year.cast(\"String\"),\n",
    "    lit(\" \"),\n",
    "    mpg_df.manufacturer,\n",
    "    lit(\" \"),\n",
    "    mpg_df.model,\n",
    "    lit(\" has a \"),\n",
    "    mpg_df.cyl.cast(\"String\"),\n",
    "    lit(\" cylinder engine.\")\n",
    "))\n",
    "\n",
    "# Transform the 'trans' column to contain either 'manual' or 'auto'\n",
    "mpg_df = mpg_df.withColumn(\"trans\", when(regexp_extract(mpg_df.trans, r\"(\\w+)\", 1) == \"manual\", \"manual\").otherwise(\"auto\"))\n",
    "\n",
    "# Show the DataFrame\n",
    "mpg_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48194ffd-a9b8-4bc8-8baa-680560e1e2d4",
   "metadata": {},
   "source": [
    "# 3. Load the tips dataset as a spark dataframe.\n",
    "\n",
    "* What percentage of observations are smokers?\n",
    "* Create a column that contains the tip percentage\n",
    "* Calculate the average tip percentage for each combination of sex and smoker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6f6ea8c-e055-4cf4-b7b2-8fd9908b03d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df = spark.createDataFrame(data('tips'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d6aa07a-80cf-487c-a2d6-65838f5e9164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n",
      "|     25.29|4.71|  Male|    No|Sun|Dinner|   4|\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|\n",
      "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|\n",
      "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|\n",
      "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|\n",
      "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|\n",
      "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|\n",
      "|     10.33|1.67|Female|    No|Sun|Dinner|   3|\n",
      "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|\n",
      "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|\n",
      "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da6bdab0-cec0-4178-a02d-dba28ef11ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of observations that are smokers: 38.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------------------+\n",
      "|   sex|smoker|avg(tip_percentage)|\n",
      "+------+------+-------------------+\n",
      "|  Male|    No|  16.06687151291298|\n",
      "|Female|    No|  15.69209707691836|\n",
      "|  Male|   Yes| 15.277117520248511|\n",
      "|Female|   Yes|  18.21503526994103|\n",
      "+------+------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Calculate the percentage of observations that are smokers\n",
    "num_smokers = tips_df.filter(col(\"smoker\") == \"Yes\").count()\n",
    "num_observations = tips_df.count()\n",
    "smokers_percentage = (num_smokers / num_observations) * 100\n",
    "\n",
    "print(\"Percentage of observations that are smokers: {:.2f}%\".format(smokers_percentage))\n",
    "\n",
    "# Create a column for tip percentage\n",
    "tips_df = tips_df.withColumn(\"tip_percentage\", (col(\"tip\") / col(\"total_bill\")) * 100)\n",
    "\n",
    "# Calculate the average tip percentage for each combination of sex and smoker\n",
    "average_tip_percentage = tips_df.groupBy(\"sex\", \"smoker\").avg(\"tip_percentage\")\n",
    "\n",
    "average_tip_percentage.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd06105-47fb-4d81-abb0-123b3b2a66ff",
   "metadata": {},
   "source": [
    "## 4. Use the seattle weather dataset referenced in the lesson to answer the questions below.\n",
    "\n",
    "* Convert the temperatures to fahrenheit.\n",
    "* Which month has the most rain, on average?\n",
    "* Which year was the windiest?\n",
    "* What is the most frequent type of weather in January?\n",
    "* What is the average high and low temperature on sunny days in July in 2013 and 2014?\n",
    "* What percentage of days were rainy in q3 of 2015?\n",
    "* For each year, find what percentage of days it rained (had non-zero precipitation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ce2c3dc-1939-4a21-94fb-092c00359dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------+--------+----+-------+\n",
      "|      date|precipitation|temp_max|temp_min|wind|weather|\n",
      "+----------+-------------+--------+--------+----+-------+\n",
      "|2012-01-01|          0.0|    12.8|     5.0| 4.7|drizzle|\n",
      "|2012-01-02|         10.9|    10.6|     2.8| 4.5|   rain|\n",
      "|2012-01-03|          0.8|    11.7|     7.2| 2.3|   rain|\n",
      "|2012-01-04|         20.3|    12.2|     5.6| 4.7|   rain|\n",
      "|2012-01-05|          1.3|     8.9|     2.8| 6.1|   rain|\n",
      "|2012-01-06|          2.5|     4.4|     2.2| 2.2|   rain|\n",
      "+----------+-------------+--------+--------+----+-------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vega_datasets import data\n",
    "\n",
    "weather = data.seattle_weather().assign(date=lambda df: df.date.astype(str))\n",
    "weather = spark.createDataFrame(weather)\n",
    "weather.show(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e270004b-0788-480e-9040-b541bd9b79dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month with the most rain, on average: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windiest year: 2012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent weather in January: fog\n",
      "Average high temperature on sunny days in July (2013 and 2014): None\n",
      "Average low temperature on sunny days in July (2013 and 2014): None\n",
      "Percentage of rainy days in Q3 of 2015: 18.478260869565215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "|year| rainy_percentage|\n",
      "+----+-----------------+\n",
      "|2012|48.36065573770492|\n",
      "|2013|41.64383561643836|\n",
      "|2014| 41.0958904109589|\n",
      "|2015|39.45205479452055|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import month, year, avg, max, count, when, sum, expr\n",
    "\n",
    "\n",
    "# Assuming you have already loaded the DataFrame as 'weather'\n",
    "\n",
    "# Convert temperatures to Fahrenheit\n",
    "weather = weather.withColumn(\"temp_max\", expr(\"temp_max * 9/5 + 32\"))\n",
    "weather = weather.withColumn(\"temp_min\", expr(\"temp_min * 9/5 + 32\"))\n",
    "\n",
    "# Which month has the most rain, on average?\n",
    "rainiest_month = weather.groupBy(month(\"date\").alias(\"month\")).avg(\"precipitation\").orderBy(\"avg(precipitation)\", ascending=False).first()[\"month\"]\n",
    "print(\"Month with the most rain, on average:\", rainiest_month)\n",
    "\n",
    "# Which year was the windiest?\n",
    "windiest_year = weather.groupBy(year(\"date\").alias(\"year\")).sum(\"wind\").orderBy(\"sum(wind)\", ascending=False).first()[\"year\"]\n",
    "print(\"Windiest year:\", windiest_year)\n",
    "\n",
    "# What is the most frequent type of weather in January?\n",
    "most_frequent_weather_january = weather.filter(month(\"date\") == 1).groupBy(\"weather\").count().orderBy(\"count\", ascending=False).first()[\"weather\"]\n",
    "print(\"Most frequent weather in January:\", most_frequent_weather_january)\n",
    "\n",
    "# What is the average high and low temperature on sunny days in July in 2013 and 2014?\n",
    "average_high_low_sunny_july = weather.filter((year(\"date\").isin([2013, 2014])) & (month(\"date\") == 7) & (weather.weather == \"sunny\")).agg(avg(\"temp_max\").alias(\"average_high\"), avg(\"temp_min\").alias(\"average_low\")).first()\n",
    "print(\"Average high temperature on sunny days in July (2013 and 2014):\", average_high_low_sunny_july[\"average_high\"])\n",
    "print(\"Average low temperature on sunny days in July (2013 and 2014):\", average_high_low_sunny_july[\"average_low\"])\n",
    "\n",
    "# What percentage of days were rainy in Q3 of 2015?\n",
    "total_days_q3_2015 = weather.filter((year(\"date\") == 2015) & (month(\"date\").between(7, 9))).count()\n",
    "rainy_days_q3_2015 = weather.filter((year(\"date\") == 2015) & (month(\"date\").between(7, 9)) & (weather.precipitation > 0)).count()\n",
    "rainy_percentage_q3_2015 = (rainy_days_q3_2015 / total_days_q3_2015) * 100\n",
    "print(\"Percentage of rainy days in Q3 of 2015:\", rainy_percentage_q3_2015)\n",
    "\n",
    "# For each year, find the percentage of days it rained (had non-zero precipitation)\n",
    "rainy_days_percentage_by_year = weather.groupBy(year(\"date\").alias(\"year\")).agg((sum(when(weather.precipitation > 0, 1)) / count(\"*\") * 100).alias(\"rainy_percentage\"))\n",
    "rainy_days_percentage_by_year.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac743956-a361-4958-b423-45adb71a0ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
